# -*- coding: utf-8 -*-
"""Major.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wSV7za8c_GQBZJD_o19aIHxj_sIePDXL
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import yfinance as yf
import pandas_datareader as web
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import plotly.graph_objects as go
plt.style.use('fivethirtyeight')

!curl -L http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz -O && tar xzvf ta-lib-0.4.0-src.tar.gz
!cd ta-lib && ./configure --prefix=/usr && make && make install && cd - && pip install ta-lib

import talib as ta

from pathlib import Path

from pandas.tseries.offsets import DateOffset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import cross_val_score, KFold

# Scikit-learn model imports
from sklearn import svm
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier

# Disable false positive error message
pd.options.mode.chained_assignment = None

import pandas_datareader as pdr

from datetime import datetime

end = datetime.now()
start = datetime(end.year-10,end.month,end.day)

aapl_df = yf.download("aapl", start, end)

aapl_df.describe()

msft_df=yf.download("MSFT", start, end)

aapl_df.head()

msft_df.head()

aapl_technical_df = aapl_df.loc["2018-01-01":, ["Close"]]

msft_technical_df = msft_df.loc["2018-01-01":, ["Close"]]

aapl_technical_df["Actual Returns"] = aapl_technical_df["Close"].pct_change()

aapl_technical_df = aapl_technical_df.dropna()

msft_technical_df["Actual Returns"] = msft_technical_df["Close"].pct_change()

msft_technical_df = msft_technical_df.dropna()

ema7 = ta.EMA(aapl_technical_df["Close"], timeperiod = 7)

ema14 = ta.EMA(aapl_technical_df["Close"], timeperiod = 14)

ema28 = ta.EMA(aapl_technical_df["Close"], timeperiod = 28)

real = ta.RSI(aapl_technical_df["Close"], timeperiod = 14)

macd, macdsignal, macdhist = ta.MACD(aapl_technical_df["Close"], fastperiod=12, slowperiod=26, signalperiod=9)

# Create a dataframe containing the indicator data
df = pd.concat({
    "7 Day EMA":ema7,
    "14 Day EMA":ema14,
    "28 Day EMA":ema28,
    "RSI":real,
    "MACD":macd,
    "MACD Signal":macdsignal,
    "MACD Hist":macdhist
}, axis=1)

# Join the original dataframe and the indicators dataframe
aapl_technical_df = aapl_technical_df.join(df)

# Drop NaN values from datframe
aapl_technical_df = aapl_technical_df.dropna()

ema7 = ta.EMA(msft_technical_df["Close"], timeperiod = 7)

ema14 = ta.EMA(msft_technical_df["Close"], timeperiod = 14)

ema28 = ta.EMA(msft_technical_df["Close"], timeperiod = 28)

real = ta.RSI(msft_technical_df["Close"], timeperiod=14)

macd, macdsignal, macdhist = ta.MACD(msft_technical_df["Close"], fastperiod=12, slowperiod=26, signalperiod=9)

# Create a dataframe containing the indicator data
df = pd.concat({
    "7 Day EMA":ema7,
    "14 Day EMA":ema14,
    "28 Day EMA":ema28,
    "RSI":real,
    "MACD":macd,
    "MACD Signal":macdsignal,
    "MACD Hist":macdhist
}, axis=1)

# Join the original dataframe and the indicators dataframe
msft_technical_df = msft_technical_df.join(df)

# Drop NaN values from datframe
msft_technical_df = msft_technical_df.dropna()

# Create signals column to store buy and sell signals - Apple
aapl_technical_df["Signals"] = 0.0

# Create signals column to store buy and sell signals - Microsoft
msft_technical_df["Signals"] = 0.0

# Create signals algorithm to identify when 7 Day EMA is above the 14 & 28 day to generate a buy signal and when 7 Day EMA is below 14 & 28 day to generate a sell signal - Apple
aapl_technical_df.loc[
    (aapl_technical_df["7 Day EMA"] > aapl_technical_df["14 Day EMA"])
    &
    (aapl_technical_df["7 Day EMA"] > aapl_technical_df["28 Day EMA"]),
    "Signals"
] = 1

aapl_technical_df.loc[
    (aapl_technical_df["7 Day EMA"] < aapl_technical_df["14 Day EMA"])
    &
    (aapl_technical_df["7 Day EMA"] > aapl_technical_df["28 Day EMA"]),
    "Signals"
] = -1

# Create signals algorithm to identify when 7 Day EMA is above the 14 & 28 day to generate a buy signal and when 7 Day EMA is below 14 & 28 day to generate a sell signal - Microsoft
msft_technical_df.loc[
    (msft_technical_df["7 Day EMA"] > msft_technical_df["14 Day EMA"])
    &
    (msft_technical_df["7 Day EMA"] > msft_technical_df["28 Day EMA"]),
    "Signals"
] = 1

msft_technical_df.loc[
    (msft_technical_df["7 Day EMA"] < msft_technical_df["14 Day EMA"])
    &
    (msft_technical_df["7 Day EMA"] > msft_technical_df["28 Day EMA"]),
    "Signals"
] = -1

# Create strategy returns column - Apple
aapl_technical_df["Strategy Returns"] = aapl_technical_df["Actual Returns"] * aapl_technical_df["Signals"].shift()

# Create strategy returns column - Microsoft
msft_technical_df["Strategy Returns"] = msft_technical_df["Actual Returns"] * msft_technical_df["Signals"].shift()

# Review cumulative return plot - Apple
(1 + aapl_technical_df[["Actual Returns", "Strategy Returns"]]).cumprod().plot(figsize=(20, 10), title="Trade Algorithm - AAPL")

# Review cumulative return plot - Microsoft
(1 + msft_technical_df[["Actual Returns", "Strategy Returns"]]).cumprod().plot(figsize=(20, 10), title="Trade Algorithm - MSFT")

# Review resulting dataframe - Apple
aapl_technical_df

# Review resulting dataframe - Microsoft
msft_technical_df

# Add signals for when 1. Close is greater than 7 Day EMA, and 2. MACD Signal is greater than MACD. Also add target variable for when Actual Returns are positive or negative
aapl_technical_df["Close_EMA"] = np.where(aapl_technical_df["Close"] > aapl_technical_df["7 Day EMA"], 1.0, -1.0)

aapl_technical_df["MACD_Signal_MACD"] = np.where(aapl_technical_df["MACD Signal"] > aapl_technical_df["MACD"], 1.0, -1.0)

aapl_technical_df["Target"] = np.where(aapl_technical_df["Actual Returns"] > 0, 1.0, 0.0)

# Drop NaN values
aapl_technical_df = aapl_technical_df.dropna().copy()

# Review dataframe
aapl_technical_df

# Create features data set X using RSI, MACD, and MACD Signals
aapl_X = aapl_technical_df[["RSI", "MACD Signal", "MACD"]]

# Create an instance of the StandardScaler
scaler = StandardScaler()

# Fit scaler to X data
X_scaler = scaler.fit(aapl_X)

# Scale X data
aapl_X_scaled = X_scaler.transform(aapl_X)

# Turn scaled data into a dataframe
aapl_X_scaled_df = pd.DataFrame(aapl_X_scaled, index=aapl_X.index, columns=aapl_X.columns)

# Review new dataframe
aapl_X_scaled_df

# Create target vector y
aapl_y = aapl_technical_df["Signals"]

# Review y data
aapl_y

# Create start and end dates for training
training_begin = aapl_technical_df.index.min()

training_end = aapl_technical_df.index.min() + DateOffset(months=6)

# Create X and y training and testing datasets
X_train = aapl_X_scaled_df.loc[training_begin:training_end]

y_train = aapl_y.loc[training_begin:training_end]

X_test = aapl_X_scaled_df.loc[training_end:]

y_test = aapl_y.loc[training_end:]

# Verify X dataframes
display(X_train.head())

display(X_test.head())

# Verify y data
display(y_train.head())

display(y_test.head())

# Define a function that runs several different models and evaluates each one to determine the most ideal models for data used
def model_selection(X, y):
    seed = 2

    models = []

    models.append(("GaussianNB", GaussianNB()))

    models.append(("SVM", svm.SVC(random_state=seed)))

    models.append(("LinearDiscriminantAnalysis", LinearDiscriminantAnalysis()))

    models.append(("LogisticRegression", LogisticRegression(random_state=seed)))

    models.append(("DecisionTreeRegressor", DecisionTreeRegressor(random_state=seed)))

    models.append(("KNeighborsClassifier", KNeighborsClassifier()))

    models.append(("DecisionTreeClassifier", DecisionTreeClassifier()))

    models.append(("RandomForestClassifier", RandomForestClassifier()))

    models.append(("ExtraTreesClassifier", ExtraTreesClassifier(random_state=seed)))

    models.append(("AdaBoostClassifier", AdaBoostClassifier(DecisionTreeClassifier(random_state=seed), random_state=seed, learning_rate=0.1)))

    models.append(("GradientBoostingClassifier", GradientBoostingClassifier(random_state=seed)))

    models.append(("MLPClassifier", MLPClassifier(random_state=seed, max_iter=2000)))

# Evaluate each model in turn
    results = []
    names = []
    scoring = "accuracy"
    for name, model in models:
        kfold = KFold(n_splits=10, shuffle=True, random_state=seed)
        cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)
        results.append(cv_results)
        names.append(name)
        msg = "%s: %0.3f (%0.3f)" % (name, cv_results.mean(), cv_results.std())
        print(msg)
    fig = plt.figure()
    fig.suptitle("Algorithm Comparison")
    ax = fig.add_subplot()
    plt.boxplot(results)
    ax.set_xticklabels(names)
    plt.xticks(rotation=90)
    plt.show()
    return results, names

# Run model selection function using scaled X and y data
model_selection(aapl_X_scaled_df, aapl_y)

#Model A - Multi-Layer Perceptron (MLP) Classifier
# Define MLPClassifier model with random state of 1 and max iterations of 2000
mlp_model = MLPClassifier(
    max_iter=2000,
    random_state=1
)

# Fit MLP model to training data
mlp_model = mlp_model.fit(X_train, y_train)

# Predict using testing data
mlp_pred = mlp_model.predict(X_test)

# Create classification report with results
mlp_testing_report = classification_report(y_test, mlp_pred)

# Print classification report
print(mlp_testing_report)

# Create predictions dataframe
mlp_predictions_df = pd.DataFrame(index=X_test.index)

# Create Predicted column using predictions
mlp_predictions_df["Predicted"] = mlp_pred

# Create Actual Returns column based on original dataframe returns column
mlp_predictions_df["Actual Returns"] = aapl_technical_df["Actual Returns"]

# Create Strategy Returns column
mlp_predictions_df["Strategy Returns"] = (mlp_predictions_df["Actual Returns"] * mlp_predictions_df["Predicted"].shift())

# Review dataframe with .head()
mlp_predictions_df.head()

# Calculate cumulative returns for Actual Returns and Strategy Returns columns
(1 + mlp_predictions_df[["Actual Returns", "Strategy Returns"]]).cumprod()

# Calculate cumulative returns for Actual Returns and Strategy Returns columns
(1 + mlp_predictions_df[["Actual Returns", "Strategy Returns"]]).cumprod()

# Plot cumulative returns for Actual Returns and Strategy Returns columns
(1 + mlp_predictions_df[["Actual Returns", "Strategy Returns"]]).cumprod().plot(figsize=(20, 10), title="MLP Classifier - AAPL")

#Model B - Extra Trees Classifier
# Define ExtraTreesClassifier model with random state of 1
etc_model = ExtraTreesClassifier(
    random_state=1
)

# Fit ETC model to training data
etc_model = etc_model.fit(X_train, y_train)

# Predict using testing data
etc_pred = etc_model.predict(X_test)

# Create classification report with results
etc_testing_report = classification_report(y_test, etc_pred)

# Print classification report
print(etc_testing_report)

# Create predictions dataframe
etc_predictions_df = pd.DataFrame(index=X_test.index)

# Create Predicted column using predictions
etc_predictions_df["Predicted"] = etc_pred

# Create Actual Returns column based on original dataframe returns column
etc_predictions_df["Actual Returns"] = aapl_technical_df["Actual Returns"]

# Create Strategy Returns column
etc_predictions_df["Strategy Returns"] = (etc_predictions_df["Actual Returns"] * etc_predictions_df["Predicted"].shift())

# Review dataframe with .head()
etc_predictions_df.head()

# Calculate cumulative returns for Actual Returns and Strategy Returns columns
(1 + etc_predictions_df[["Actual Returns", "Strategy Returns"]]).cumprod()

# Plot cumulative returns for Actual Returns and Strategy Returns columns
(1 + etc_predictions_df[["Actual Returns", "Strategy Returns"]]).cumprod().plot(figsize=(20, 10), title="ETC Classifier - AAPL")

#Model C - Gradient Boosting Classifier

# Define GradientBoostingClassifier model with random state of 1
gbc_model = GradientBoostingClassifier(
    random_state=1
)

# Fit GBC model to training data
gbc_model = gbc_model.fit(X_train, y_train)

# Predict using testing data
gbc_pred = gbc_model.predict(X_test)

# Create classification report with results
gbc_testing_report = classification_report(y_test, gbc_pred)

# Print classification report
print(gbc_testing_report)

# Create predictions dataframe
gbc_predictions_df = pd.DataFrame(index=X_test.index)

# Create Predicted column using predictions
gbc_predictions_df["Predicted"] = gbc_pred

# Create Actual Returns column based on original dataframe returns column
gbc_predictions_df["Actual Returns"] = aapl_technical_df["Actual Returns"]

# Create Strategy Returns column
gbc_predictions_df["Strategy Returns"] = (gbc_predictions_df["Actual Returns"] * gbc_predictions_df["Predicted"].shift())

# Review dataframe with .head()
gbc_predictions_df.head()

# Calculate cumulative returns for Actual Returns and Strategy Returns columns
(1 + gbc_predictions_df[["Actual Returns", "Strategy Returns"]]).cumprod()

# Plot cumulative returns for Actual Returns and Strategy Returns columns
(1 + gbc_predictions_df[["Actual Returns", "Strategy Returns"]]).cumprod().plot(figsize=(20, 10), title="GBC Classifier - AAPL")

#Stock B Models - MSFT
#Data Prep
# Add signals for when 1. Close is greater than 7 Day EMA, and 2. MACD Signal is greater than MACD. Also add target variable for when Actual Returns are positive or negative
msft_technical_df["Close_EMA"] = np.where(msft_technical_df["Close"] > msft_technical_df["7 Day EMA"], 1.0, -1.0)

msft_technical_df["MACD_Signal_MACD"] = np.where(msft_technical_df["MACD Signal"] > msft_technical_df["MACD"], 1.0, -1.0)

msft_technical_df["Target"] = np.where(msft_technical_df["Actual Returns"] > 0, 1.0, 0.0)

# Drop NaN values
msft_technical_df = msft_technical_df.dropna().copy()

# Review dataframe
msft_technical_df

# Create features data set X using RSI, MACD, and MACD Signals
msft_X = msft_technical_df[["RSI", "MACD Signal", "MACD"]]

# Create an instance of the StandardScaler
scaler = StandardScaler()

# Fit scaler to X data
X_scaler = scaler.fit(msft_X)

# Scale X data
msft_X_scaled = X_scaler.transform(msft_X)

# Turn scaled data into a dataframe
msft_X_scaled_df = pd.DataFrame(msft_X_scaled, index=msft_X.index, columns=msft_X.columns)

# Review new dataframe
msft_X_scaled_df

# Create target vector y
msft_y = msft_technical_df["Signals"]

# Review y data
msft_y

# Create start and end dates for model test
begin = msft_technical_df.index.min()

end = msft_technical_df.index.max()

# Create X and y for model
X_msft = msft_X_scaled_df.loc[begin:end]

y_msft = msft_y.loc[begin:end]

# Verify X dataframe
X_msft.head()

# Verify y data
y_msft[:10]

#Model A - Multi-Layer Perceptron (MLP) Classifier
# Predict using Microsoft data and Apple model
mlp_pred_msft = mlp_model.predict(X_msft)

# Create classification report with results
mlp_testing_report_msft = classification_report(y_msft, mlp_pred_msft)

# Print classification report
print(mlp_testing_report_msft)

# Create predictions dataframe
mlp_predictions_msft_df = pd.DataFrame(index=X_msft.index)

# Create Predicted column using predictions
mlp_predictions_msft_df["Predicted"] = mlp_pred_msft

# Create Actual Returns column based on original dataframe returns column
mlp_predictions_msft_df["Actual Returns"] = msft_technical_df["Actual Returns"]

# Create Strategy Returns column
mlp_predictions_msft_df["Strategy Returns"] = (mlp_predictions_msft_df["Actual Returns"] * mlp_predictions_msft_df["Predicted"].shift())

# Review dataframe with .head()
mlp_predictions_msft_df.head()

# Plot cumulative returns for Actual Returns and Strategy Returns columns
(1 + mlp_predictions_msft_df[["Actual Returns", "Strategy Returns"]]).cumprod()

# Plot cumulative returns for Actual Returns and Strategy Returns columns
(1 + mlp_predictions_msft_df[["Actual Returns", "Strategy Returns"]]).cumprod().plot(figsize=(20, 10), title="MLP Classifier - MSFT")

#Model B - Extra Trees Classifier
# Predict using Microsoft data and Apple model
etc_pred_msft = etc_model.predict(X_msft)

# Create classification report with results
etc_testing_report_msft = classification_report(y_msft, etc_pred_msft)

# Print classification report
print(etc_testing_report_msft)

# Create predictions dataframe
etc_predictions_msft_df = pd.DataFrame(index=X_msft.index)

# Create Predicted column using predictions
etc_predictions_msft_df["Predicted"] = etc_pred_msft

# Create Actual Returns column based on original dataframe returns column
etc_predictions_msft_df["Actual Returns"] = msft_technical_df["Actual Returns"]

# Create Strategy Returns column
etc_predictions_msft_df["Strategy Returns"] = (etc_predictions_msft_df["Actual Returns"] * etc_predictions_msft_df["Predicted"].shift())

# Review dataframe with .head()
etc_predictions_msft_df.head()

# Plot cumulative returns for Actual Returns and Strategy Returns columns
(1 + etc_predictions_msft_df[["Actual Returns", "Strategy Returns"]]).cumprod()

# Plot cumulative returns for Actual Returns and Strategy Returns columns
(1 + etc_predictions_msft_df[["Actual Returns", "Strategy Returns"]]).cumprod().plot(figsize=(20, 10), title="ETC Classifier - MSFT")





## Model C - Gradient Boosting Classifier
# Predict using Microsoft data and Apple model
gbc_pred_msft = gbc_model.predict(X_msft)

# Create classification report with results
gbc_testing_report_msft = classification_report(y_msft, gbc_pred_msft)

# Print classification report
print(gbc_testing_report_msft)

# Create predictions dataframe
gbc_predictions_msft_df = pd.DataFrame(index=X_msft.index)

# Create Predicted column using predictions
gbc_predictions_msft_df["Predicted"] = gbc_pred_msft

# Create Actual Returns column based on original dataframe returns column
gbc_predictions_msft_df["Actual Returns"] = msft_technical_df["Actual Returns"]

# Create Strategy Returns column
gbc_predictions_msft_df["Strategy Returns"] = (gbc_predictions_msft_df["Actual Returns"] * gbc_predictions_msft_df["Predicted"].shift())

# Review dataframe with .head()
gbc_predictions_msft_df.head()

# Plot cumulative returns for Actual Returns and Strategy Returns columns
(1 + gbc_predictions_msft_df[["Actual Returns", "Strategy Returns"]]).cumprod()

# Plot cumulative returns for Actual Returns and Strategy Returns columns
(1 + gbc_predictions_msft_df[["Actual Returns", "Strategy Returns"]]).cumprod().plot(figsize=(20, 10), title="GBC Classifier - MSFT")

from patternpy.tradingpatterns import head_and_shoulders

# Have price data (OCHLV dataframe)
df = pd.Dataframe(aapl)

# Apply pattern indicator screener
df = head_and_shoulders(df)

# New column head_shoulder_pattern is created with entries containing either: NaN, 'Head and Shoulder' or 'Inverse Head and Shoulder'
print(df)

import pandas as pd
import numpy as np


def detect_head_shoulder(df, window=3):
# Define the rolling window
    roll_window = window
    # Create a rolling window for High and Low
    df['high_roll_max'] = df['High'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['Low'].rolling(window=roll_window).min()
    # Create a boolean mask for Head and Shoulder pattern
    mask_head_shoulder = ((df['high_roll_max'] > df['High'].shift(1)) & (df['high_roll_max'] > df['High'].shift(-1)) & (df['High'] < df['High'].shift(1)) & (df['High'] < df['High'].shift(-1)))
    # Create a boolean mask for Inverse Head and Shoulder pattern
    mask_inv_head_shoulder = ((df['low_roll_min'] < df['Low'].shift(1)) & (df['low_roll_min'] < df['Low'].shift(-1)) & (df['Low'] > df['Low'].shift(1)) & (df['Low'] > df['Low'].shift(-1)))
    # Create a new column for Head and Shoulder and its inverse pattern and populate it using the boolean masks
    df['head_shoulder_pattern'] = np.nan
    df.loc[mask_head_shoulder, 'head_shoulder_pattern'] = 'Head and Shoulder'
    df.loc[mask_inv_head_shoulder, 'head_shoulder_pattern'] = 'Inverse Head and Shoulder'
    return df
    # return not df['head_shoulder_pattern'].isna().any().item()

def detect_multiple_tops_bottoms(df, window=3):
# Define the rolling window
    roll_window = window
    # Create a rolling window for High and Low
    df['high_roll_max'] = df['High'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['Low'].rolling(window=roll_window).min()
    df['close_roll_max'] = df['Close'].rolling(window=roll_window).max()
    df['close_roll_min'] = df['Close'].rolling(window=roll_window).min()
    # Create a boolean mask for multiple top pattern
    mask_top = (df['high_roll_max'] >= df['High'].shift(1)) & (df['close_roll_max'] < df['Close'].shift(1))
    # Create a boolean mask for multiple bottom pattern
    mask_bottom = (df['low_roll_min'] <= df['Low'].shift(1)) & (df['close_roll_min'] > df['Close'].shift(1))
    # Create a new column for multiple top bottom pattern and populate it using the boolean masks
    df['multiple_top_bottom_pattern'] = np.nan
    df.loc[mask_top, 'multiple_top_bottom_pattern'] = 'Multiple Top'
    df.loc[mask_bottom, 'multiple_top_bottom_pattern'] = 'Multiple Bottom'
    return df

def calculate_support_resistance(df, window=3):
# Define the rolling window
    roll_window = window
    # Set the number of standard deviation
    std_dev = 2
    # Create a rolling window for High and Low
    df['high_roll_max'] = df['High'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['Low'].rolling(window=roll_window).min()
    # Calculate the mean and standard deviation for High and Low
    mean_high = df['High'].rolling(window=roll_window).mean()
    std_high = df['High'].rolling(window=roll_window).std()
    mean_low = df['Low'].rolling(window=roll_window).mean()
    std_low = df['Low'].rolling(window=roll_window).std()
    # Create a new column for support and resistance
    df['support'] = mean_low - std_dev * std_low
    df['resistance'] = mean_high + std_dev * std_high
    return df
def detect_triangle_pattern(df, window=3):
    # Define the rolling window
    roll_window = window
    # Create a rolling window for High and Low
    df['high_roll_max'] = df['High'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['Low'].rolling(window=roll_window).min()
    # Create a boolean mask for ascending triangle pattern
    mask_asc = (df['high_roll_max'] >= df['High'].shift(1)) & (df['low_roll_min'] <= df['Low'].shift(1)) & (df['Close'] > df['Close'].shift(1))
    # Create a boolean mask for descending triangle pattern
    mask_desc = (df['high_roll_max'] <= df['High'].shift(1)) & (df['low_roll_min'] >= df['Low'].shift(1)) & (df['Close'] < df['Close'].shift(1))
    # Create a new column for triangle pattern and populate it using the boolean masks
    df['triangle_pattern'] = np.nan
    df.loc[mask_asc, 'triangle_pattern'] = 'Ascending Triangle'
    df.loc[mask_desc, 'triangle_pattern'] = 'Descending Triangle'
    return df

def detect_wedge(df, window=3):
    # Define the rolling window
    roll_window = window
    # Create a rolling window for High and Low
    df['high_roll_max'] = df['High'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['Low'].rolling(window=roll_window).min()
    df['trend_high'] = df['High'].rolling(window=roll_window).apply(lambda x: 1 if (x[-1]-x[0])>0 else -1 if (x[-1]-x[0])<0 else 0)
    df['trend_low'] = df['Low'].rolling(window=roll_window).apply(lambda x: 1 if (x[-1]-x[0])>0 else -1 if (x[-1]-x[0])<0 else 0)
    # Create a boolean mask for Wedge Up pattern
    mask_wedge_up = (df['high_roll_max'] >= df['High'].shift(1)) & (df['low_roll_min'] <= df['Low'].shift(1)) & (df['trend_high'] == 1) & (df['trend_low'] == 1)
    # Create a boolean mask for Wedge Down pattern
        # Create a boolean mask for Wedge Down pattern
    mask_wedge_down = (df['high_roll_max'] <= df['High'].shift(1)) & (df['low_roll_min'] >= df['Low'].shift(1)) & (df['trend_high'] == -1) & (df['trend_low'] == -1)
    # Create a new column for Wedge Up and Wedge Down pattern and populate it using the boolean masks
    df['wedge_pattern'] = np.nan
    df.loc[mask_wedge_up, 'wedge_pattern'] = 'Wedge Up'
    df.loc[mask_wedge_down, 'wedge_pattern'] = 'Wedge Down'
    return df
def detect_channel(df, window=3):
    # Define the rolling window
    roll_window = window
    # Define a factor to check for the range of channel
    channel_range = 0.1
    # Create a rolling window for High and Low
    df['high_roll_max'] = df['High'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['Low'].rolling(window=roll_window).min()
    df['trend_high'] = df['High'].rolling(window=roll_window).apply(lambda x: 1 if (x[-1]-x[0])>0 else -1 if (x[-1]-x[0])<0 else 0)
    df['trend_low'] = df['Low'].rolling(window=roll_window).apply(lambda x: 1 if (x[-1]-x[0])>0 else -1 if (x[-1]-x[0])<0 else 0)
    # Create a boolean mask for Channel Up pattern
    mask_channel_up = (df['high_roll_max'] >= df['High'].shift(1)) & (df['low_roll_min'] <= df['Low'].shift(1)) & (df['high_roll_max'] - df['low_roll_min'] <= channel_range * (df['high_roll_max'] + df['low_roll_min'])/2) & (df['trend_high'] == 1) & (df['trend_low'] == 1)
    # Create a boolean mask for Channel Down pattern
    mask_channel_down = (df['high_roll_max'] <= df['High'].shift(1)) & (df['low_roll_min'] >= df['Low'].shift(1)) & (df['high_roll_max'] - df['low_roll_min'] <= channel_range * (df['high_roll_max'] + df['low_roll_min'])/2) & (df['trend_high'] == -1) & (df['trend_low'] == -1)
    # Create a new column for Channel Up and Channel Down pattern and populate it using the boolean masks
    df['channel_pattern'] = np.nan
    df.loc[mask_channel_up, 'channel_pattern'] = 'Channel Up'
    df.loc[mask_channel_down, 'channel_pattern'] = 'Channel Down'
    return df

def detect_double_top_bottom(df, window=3, threshold=0.05):
    # Define the rolling window
    roll_window = window
    # Define a threshold to check for the range of pattern
    range_threshold = threshold

    # Create a rolling window for High and Low
    df['high_roll_max'] = df['High'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['Low'].rolling(window=roll_window).min()

    # Create a boolean mask for Double Top pattern
    mask_double_top = (df['high_roll_max'] >= df['High'].shift(1)) & (df['high_roll_max'] >= df['High'].shift(-1)) & (df['High'] < df['High'].shift(1)) & (df['High'] < df['High'].shift(-1)) & ((df['High'].shift(1) - df['Low'].shift(1)) <= range_threshold * (df['High'].shift(1) + df['Low'].shift(1))/2) & ((df['High'].shift(-1) - df['Low'].shift(-1)) <= range_threshold * (df['High'].shift(-1) + df['Low'].shift(-1))/2)
    # Create a boolean mask for Double Bottom pattern
    mask_double_bottom = (df['low_roll_min'] <= df['Low'].shift(1)) & (df['low_roll_min'] <= df['Low'].shift(-1)) & (df['Low'] > df['Low'].shift(1)) & (df['Low'] > df['Low'].shift(-1)) & ((df['High'].shift(1) - df['Low'].shift(1)) <= range_threshold * (df['High'].shift(1) + df['Low'].shift(1))/2) & ((df['High'].shift(-1) - df['Low'].shift(-1)) <= range_threshold * (df['High'].shift(-1) + df['Low'].shift(-1))/2)

    # Create a new column for Double Top and Double Bottom pattern and populate it using the boolean masks
    df['double_pattern'] = np.nan
    df.loc[mask_double_top, 'double_pattern'] = 'Double Top'
    df.loc[mask_double_bottom, 'double_pattern'] = 'Double Bottom'

    return df

def detect_trendline(df, window=2):
    # Define the rolling window
    roll_window = window
    # Create new columns for the linear regression slope and y-intercept
    df['slope'] = np.nan
    df['intercept'] = np.nan

    for i in range(window, len(df)):
        x = np.array(range(i-window, i))
        y = df['Close'][i-window:i]
        A = np.vstack([x, np.ones(len(x))]).T
        m, c = np.linalg.lstsq(A, y, rcond=None)[0]
        df.at[df.index[i], 'slope'] = m
        df.at[df.index[i], 'intercept'] = c

    # Create a boolean mask for trendline support
    mask_support = df['slope'] > 0

    # Create a boolean mask for trendline resistance
    mask_resistance = df['slope'] < 0

    # Create new columns for trendline support and resistance
    df['support'] = np.nan
    df['resistance'] = np.nan

    # Populate the new columns using the boolean masks
    df.loc[mask_support, 'support'] = df['Close'] * df['slope'] + df['intercept']
    df.loc[mask_resistance, 'resistance'] = df['Close'] * df['slope'] + df['intercept']

    return df

def find_pivots(df):
    # Calculate differences between consecutive highs and lows
    high_diffs = df['high'].diff()
    low_diffs = df['low'].diff()

    # Find higher high
    higher_high_mask = (high_diffs > 0) & (high_diffs.shift(-1) < 0)

    # Find lower low
    lower_low_mask = (low_diffs < 0) & (low_diffs.shift(-1) > 0)

    # Find lower high
    lower_high_mask = (high_diffs < 0) & (high_diffs.shift(-1) > 0)

    # Find higher low
    higher_low_mask = (low_diffs > 0) & (low_diffs.shift(-1) < 0)

    # Create signals column
    df['signal'] = ''
    df.loc[higher_high_mask, 'signal'] = 'HH'
    df.loc[lower_low_mask, 'signal'] = 'LL'
    df.loc[lower_high_mask, 'signal'] = 'LH'
    df.loc[higher_low_mask, 'signal'] = 'HL'

df = yf.download("aapl", start, end)


print (detect_head_shoulder(df, window=3))

df_h = df[df['head_shoulder_pattern']=="Head and Shoulder"]

df_h.head()



import

